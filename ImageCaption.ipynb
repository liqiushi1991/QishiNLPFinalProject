{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build caption vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import os\n",
    "from collections       import Counter\n",
    "from pycocotools.coco  import COCO\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def build_vocab(json, threshold):\n",
    "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "    coco = COCO(json)\n",
    "    counter = Counter()\n",
    "    ids = coco.anns.keys()\n",
    "    for i, id in enumerate(ids):\n",
    "        caption = str(coco.anns[id]['caption'])\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "        counter.update(tokens)\n",
    "\n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(\"[{}/{}] Tokenized the captions.\".format(i+1, len(ids)))\n",
    "\n",
    "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "\n",
    "    # Create a vocab wrapper and add some special tokens.\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<start>')\n",
    "    vocab.add_word('<end>')\n",
    "    vocab.add_word('<unk>')\n",
    "\n",
    "    # Add the words to the vocabulary.\n",
    "    for i, word in enumerate(words):\n",
    "        vocab.add_word(word)\n",
    "    return vocab\n",
    "\n",
    "def save_vocab(kwargs, overwrite=False):\n",
    "    vocab_path = kwargs.get('vocab_path', None)\n",
    "    exists = os.path.isfile(vocab_path)\n",
    "    \n",
    "    if not exists or overwrite:\n",
    "        vocab = build_vocab(json=kwargs.get('caption_path', None), threshold=kwargs.get('threshold', None))\n",
    "        with open(vocab_path, 'wb') as f:\n",
    "            pickle.dump(vocab, f)\n",
    "        print(\"Total vocabulary size: {}\".format(len(vocab)))\n",
    "        print(\"Saved the vocabulary wrapper to '{}'\".format(vocab_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {}\n",
    "kwargs['caption_path'] = './data/annotations/captions_val2017.json' # path for train annotation file'\n",
    "kwargs['vocab_path'] = './data/vocab.pkl' # path for saving vocabulary wrapper\n",
    "kwargs['threshold'] = 4 # minimum word count threshold'\n",
    "save_vocab(kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Resize image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def resize_images(image_dir, output_dir, size):\n",
    "    \"\"\"Resize the images in 'image_dir' and save into 'output_dir'.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    images = os.listdir(image_dir)\n",
    "    num_images = len(images)\n",
    "    for i, image in enumerate(images):\n",
    "        img_output_dir = os.path.join(output_dir, image)\n",
    "        if os.path.exists(img_output_dir):\n",
    "            continue\n",
    "            \n",
    "        with open(os.path.join(image_dir, image), 'r+b') as f:\n",
    "            with Image.open(f) as img:\n",
    "                img = img.resize(size, Image.ANTIALIAS)\n",
    "                img.save(img_output_dir, img.format)\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"[{}/{}] Resized the images and saved into '{}'.\"\n",
    "                   .format(i+1, num_images, output_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = './data/val2017/' # directory for train images'\n",
    "output_dir = './data/resizedval2017/' # directory for saving resized images'\n",
    "image_size = (256, 256) \n",
    "args = resize_images(image_dir, output_dir, image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load precessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From pytorch tutorial\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class CocoDataset(data.Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self, root, json, vocab, transform=None):\n",
    "        \"\"\"Set the path for images, captions and vocabulary wrapper.\n",
    "        \n",
    "        Args:\n",
    "            root: image directory.\n",
    "            json: coco annotation file path.\n",
    "            vocab: vocabulary wrapper.\n",
    "            transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.coco = COCO(json)\n",
    "        self.ids = list(self.coco.anns.keys())\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
    "        coco = self.coco\n",
    "        vocab = self.vocab\n",
    "        ann_id = self.ids[index]\n",
    "        caption = coco.anns[ann_id]['caption']\n",
    "        img_id = coco.anns[ann_id]['image_id']\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        image = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert caption (string) to word ids.\n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption = []\n",
    "        caption.append(vocab('<start>'))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab('<end>'))\n",
    "        target = torch.Tensor(caption)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging caption (including padding) is not supported in default.\n",
    "\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]        \n",
    "    return images, targets, lengths\n",
    "\n",
    "def get_loader(root, json, vocab, transform, batch_size, shuffle, num_workers):\n",
    "    \"\"\"Returns torch.utils.data.DataLoader for custom coco dataset.\"\"\"\n",
    "    # COCO caption dataset\n",
    "    coco = CocoDataset(root=root,\n",
    "                       json=json,\n",
    "                       vocab=vocab,\n",
    "                       transform=transform)\n",
    "    \n",
    "    # Data loader for COCO dataset\n",
    "    # This will return (images, captions, lengths) for each iteration.\n",
    "    # images: a tensor of shape (batch_size, 3, 224, 224).\n",
    "    # captions: a tensor of shape (batch_size, padded_length).\n",
    "    # lengths: a list indicating valid length for each caption. length is (batch_size).\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=coco, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=shuffle,\n",
    "                                              num_workers=num_workers,\n",
    "                                              collate_fn=collate_fn)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn import init\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_size, encoder=models.resnet50):\n",
    "        super(Encoder, self).__init__()\n",
    "        if 'resnet' in encoder.__module__:\n",
    "            resnet = encoder(pretrained=True)\n",
    "            modules = list(resnet.children())[:-2]\n",
    "            self.dim = 2048\n",
    "        elif 'vgg' in encoder.__module__:\n",
    "            resnet = encoder(pretrained=True)\n",
    "            modules = list(resnet.children())[:-1]\n",
    "            self.dim = 512\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract the image feature vectors.\"\"\"\n",
    "        features = self.resnet(images) # [batch, self.dim, 7, 7]\n",
    "        features = features.permute(0, 2, 3, 1)\n",
    "        features = features.view(features.size(0), -1, features.size(-1)) # [batch, 49, self.dim=2048]\n",
    "        return features\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, hidden_size=512):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.affine_W = nn.Linear(encoder_dim, hidden_size)\n",
    "        self.affine_U = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.affine_V = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init.xavier_uniform(self.affine_W.weight )\n",
    "        init.xavier_uniform(self.affine_U.weight )\n",
    "        init.xavier_uniform(self.affine_V.weight )\n",
    "\n",
    "    def forward(self, a, prev_hidden_state):\n",
    "        att = torch.tanh(self.affine_W(a) + self.affine_U(prev_hidden_state).unsqueeze(1)) # [batch, 49, 1]\n",
    "        e_t = self.affine_V(att).squeeze(2)\n",
    "\n",
    "        alpha_t = nn.Softmax(1)(e_t) # [batch, 49]\n",
    "        context_t = (a * alpha_t.unsqueeze(2)).sum(1) # [batch, 2048]\n",
    "\n",
    "        return context_t, alpha_t\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, encoder_dim, vocab_size, hidden_size=512):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.encoder_dim = encoder_dim\n",
    "\n",
    "        self.init_affine_h = nn.Linear(encoder_dim, hidden_size)\n",
    "        self.init_affine_c = nn.Linear(encoder_dim, hidden_size)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.attention = Attention(encoder_dim, hidden_size=hidden_size)\n",
    "        self.f_beta = nn.Linear(hidden_size, encoder_dim)\n",
    "        self.lstm = nn.LSTMCell(hidden_size+encoder_dim, hidden_size)\n",
    "        self.output_W = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        batch_size = features.size(0)\n",
    "\n",
    "        features_avg = features.mean(dim=1)\n",
    "        h = torch.tanh(self.init_affine_h(features_avg))\n",
    "        c = torch.tanh(self.init_affine_c(features_avg))\n",
    "\n",
    "        T = max([len(caption) for caption in captions])\n",
    "        prev_word = torch.zeros(batch_size, 1).long()\n",
    "        pred_words = torch.zeros(batch_size, T, self.vocab_size) # [128, 26, 2699]\n",
    "        alphas = torch.zeros(batch_size, T, features.size(1))\n",
    "\n",
    "        embedding = self.embedding(prev_word)\n",
    "\n",
    "        for t in range(T):\n",
    "            context_t, alpha_t = self.attention.forward(features, h)\n",
    "            gate = torch.sigmoid(self.f_beta(h))\n",
    "            gated_context = gate * context_t\n",
    "            \n",
    "            # dim = 3 is for mini batch\n",
    "            embedding = embedding.squeeze(1) if embedding.dim() == 3 else embedding\n",
    "            lstm_input = torch.cat((embedding, gated_context), dim=1)\n",
    "\n",
    "            h, c = self.lstm(lstm_input, (h, c))\n",
    "            output = torch.sigmoid(self.output_W(h))\n",
    "\n",
    "            pred_words[:, t] = output\n",
    "            alphas[:, t] = alpha_t\n",
    "            if not self.training:\n",
    "                embedding = self.embedding(output.max(1)[1].reshape(batch_size, 1))\n",
    "            else:\n",
    "                prev_word = captions[:,t]\n",
    "                embedding = self.embedding(prev_word)\n",
    "        return pred_words, alphas\n",
    "\n",
    "    def caption(self, features, beam_size):\n",
    "        '''\n",
    "        From https://github.com/kelvinxu/arctic-captions/blob/master/generate_caps.py        \n",
    "        '''\n",
    "        \n",
    "        prev_words = torch.zeros(beam_size, 1).long()\n",
    "\n",
    "        sentences = prev_words\n",
    "        top_preds = torch.zeros(beam_size, 1)\n",
    "        alphas = torch.ones(beam_size, 1, features.size(1))\n",
    "\n",
    "        completed_sentences = []\n",
    "        completed_sentences_alphas = []\n",
    "        completed_sentences_preds = []\n",
    "\n",
    "        step = 1\n",
    "        h = torch.tanh(self.init_affine_h(features_avg))\n",
    "        c = torch.tanh(self.init_affine_c(features_avg))\n",
    "\n",
    "        while True:\n",
    "            embedding = self.embedding(prev_words).squeeze(1)\n",
    "            context, alpha = self.attention(img_features, h)\n",
    "            gate = torch.sigmoid(self.f_beta(h))\n",
    "            gated_context = gate * context\n",
    "\n",
    "            lstm_input = torch.cat((embedding, gated_context), dim=1)\n",
    "            h, c = self.lstm(lstm_input, (h, c))\n",
    "            output = top_preds.expand_as(output) + output\n",
    "\n",
    "            if step == 1:\n",
    "                top_preds, top_words = output[0].topk(beam_size, 0, True, True)\n",
    "            else:\n",
    "                top_preds, top_words = output.view(-1).topk(beam_size, 0, True, True)\n",
    "            prev_word_idxs = top_words / output.size(1)\n",
    "            next_word_idxs = top_words % output.size(1)\n",
    "\n",
    "            sentences = torch.cat((sentences[prev_word_idxs], next_word_idxs.unsqueeze(1)), dim=1)\n",
    "            alphas = torch.cat((alphas[prev_word_idxs], alpha[prev_word_idxs].unsqueeze(1)), dim=1)\n",
    "\n",
    "            incomplete = [idx for idx, next_word in enumerate(next_word_idxs) if next_word != 1]\n",
    "            complete = list(set(range(len(next_word_idxs))) - set(incomplete))\n",
    "\n",
    "            if len(complete) > 0:\n",
    "                completed_sentences.extend(sentences[complete].tolist())\n",
    "                completed_sentences_alphas.extend(alphas[complete].tolist())\n",
    "                completed_sentences_preds.extend(top_preds[complete])\n",
    "            beam_size -= len(complete)\n",
    "\n",
    "            if beam_size == 0:\n",
    "                break\n",
    "            sentences = sentences[incomplete]\n",
    "            alphas = alphas[incomplete]\n",
    "            h = h[prev_word_idxs[incomplete]]\n",
    "            c = c[prev_word_idxs[incomplete]]\n",
    "            features = features[prev_word_idxs[incomplete]]\n",
    "            top_preds = top_preds[incomplete].unsqueeze(1)\n",
    "            prev_words = next_word_idxs[incomplete].unsqueeze(1)\n",
    "\n",
    "            if step > 50:\n",
    "                break\n",
    "            step += 1\n",
    "\n",
    "        idx = completed_sentences_preds.index(max(completed_sentences_preds))\n",
    "        sentence = completed_sentences[idx]\n",
    "        alpha = completed_sentences_alphas[idx]\n",
    "        return sentence, alpha\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class AverageMeter(object):\n",
    "    '''From https://github.com/pytorch/examples/blob/master/imagenet/main.py'''\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(preds, targets, k):\n",
    "    batch_size = targets.size(0)\n",
    "    _, pred = preds.topk(k, 1, True, True)\n",
    "    pred = pred.permute(1, 0, 2)\n",
    "    correct = pred.eq(targets.expand_as(pred))\n",
    "    correct_total = correct.view(-1).float().sum()\n",
    "    print(correct_total, correct.numel())\n",
    "    return correct_total.item() / float(correct.numel() / k)\n",
    "\n",
    "def calculate_caption_lengths(word_dict, captions):\n",
    "    lengths = 0\n",
    "    for caption_tokens in captions:\n",
    "        for token in caption_tokens:\n",
    "            if token in (word_dict['<start>'], word_dict['<end>'], word_dict['<pad>']):\n",
    "                continue\n",
    "            else:\n",
    "                lengths += 1\n",
    "    return lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cpu')\n",
    "\n",
    "def train(model_path, crop_size, vocab_path, image_dir, caption_path, log_step=10,\n",
    "      save_step=1000, embed_size=256, hidden_size=512, num_epochs=5,\n",
    "      batch_size=128, num_workers=2, learning_rate=0.001, alpha_c=1):\n",
    "    # Create model directory\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    # Image preprocessing, normalization for the pretrained resnet\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomCrop(crop_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # used for calculating bleu scores\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    \n",
    "    # Load vocabulary wrapper\n",
    "    with open(vocab_path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "    # Build data loader\n",
    "    data_loader = get_loader(image_dir, caption_path, vocab,\n",
    "                             transform, batch_size,\n",
    "                             shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    # Build the models\n",
    "    encoder = Encoder(models.vgg19).to(device)\n",
    "    decoder = Decoder(encoder.dim, len(vocab), hidden_size=hidden_size).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    params = list(decoder.parameters())\n",
    "    optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "    # Train the models\n",
    "    total_step = len(data_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "\n",
    "            # Set mini-batch dataset\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward, backward and optimize\n",
    "            features = encoder.forward(images)\n",
    "            prediction, alphas = decoder.forward(features, captions)\n",
    "\n",
    "            att_regularization = alpha_c * ((1 - alphas.sum(1))**2).mean()\n",
    "            loss = criterion(prediction.permute(0,2,1), captions) + att_regularization\n",
    "            decoder.zero_grad()\n",
    "            encoder.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_caption_length = calculate_caption_lengths(vocab.word2idx, captions)\n",
    "            acc1 = accuracy(prediction.permute(0,2,1), captions, 1)\n",
    "            acc5 = accuracy(prediction.permute(0,2,1), captions, 5)\n",
    "            losses.update(loss.item(), total_caption_length)\n",
    "            top1.update(acc1, total_caption_length)\n",
    "            top5.update(acc5, total_caption_length)\n",
    "\n",
    "            # Print log info\n",
    "            if i % log_step == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                      .format(epoch, num_epochs, i, total_step, loss.item(), np.exp(loss.item())))\n",
    "                print('Top 1 Accuracy {top1.val:.3f} ({top1.avg:.3f}), Top 5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                        top1=top1, top5=top5))\n",
    "            # Save the model checkpoints\n",
    "            if (i + 1) % save_step == 0:\n",
    "                torch.save(decoder.state_dict(), os.path.join(\n",
    "                    model_path, 'decoder-{}-{}.ckpt'.format(epoch + 1, i + 1)))\n",
    "                torch.save(encoder.state_dict(), os.path.join(\n",
    "                    model_path, 'encoder-{}-{}.ckpt'.format(epoch + 1, i + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "tensor(0.) 2560\n",
      "tensor(3.) 12800\n",
      "Epoch [0/5], Step [0/196], Loss: 8.2578, Perplexity: 3857.5028\n",
      "Top 1 Accuracy 0.000 (0.000), Top 5 Accuracy 0.001 (0.001)\n",
      "tensor(2914.) 4480\n",
      "tensor(3191.) 22400\n",
      "tensor(1756.) 3328\n",
      "tensor(2098.) 16640\n"
     ]
    }
   ],
   "source": [
    "model_path = './models/'  # path for saving trained models\n",
    "crop_size = 224  # size for randomly cropping images\n",
    "vocab_path = './data/vocab.pkl'  # path for vocabulary wrapper\n",
    "image_dir = './data/resizedval2017'\n",
    "caption_path = './data/annotations/captions_val2017.json'\n",
    "train(model_path, crop_size, vocab_path, image_dir, caption_path, log_step=10,\n",
    "      save_step=1000, embed_size=256, hidden_size=512, num_epochs=5,\n",
    "      batch_size=128, num_workers=2, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model_path, crop_size, vocab_path, image_dir, caption_path, log_step=10):\n",
    "    raise NotImplementError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
